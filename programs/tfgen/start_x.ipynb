{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"images/etcbc.png\"/>\n",
    "<img align=\"right\" src=\"images/peshitta_small.png\"/>\n",
    "<img align=\"right\" src=\"images/tf-small.png\"/>\n",
    "\n",
    "\n",
    "# Tutorial\n",
    "\n",
    "This notebook gets you started with using\n",
    "[Text-Fabric](https://github.com/Dans-labs/text-fabric) for coding in Syriac texts.\n",
    "\n",
    "Chances are that a bit of reading about the underlying\n",
    "[data model](https://github.com/Dans-labs/text-fabric/wiki/Data-model)\n",
    "helps you to follow the exercises below, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most programs start with loading a few modules.\n",
    "In the next cell, the first line loads standard modules that come with Python itself,\n",
    "and the second cell loads Text-Fabric.\n",
    "\n",
    "Before you can run this, you need to install it.\n",
    "The basic instruction for that is, on a terminal:\n",
    "\n",
    "```\n",
    "pip install text-fabric\n",
    "```\n",
    "\n",
    "if you have installed Python with the help of Anaconda, or\n",
    "\n",
    "```\n",
    "sudo -H pip3 install text-fabric\n",
    "```\n",
    "if you have installed Python from [python.org](https://www.python.org).\n",
    "\n",
    "Make sure that you do all this with Python **3**, not 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:42:40.232770Z",
     "start_time": "2018-02-16T15:42:40.213212Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os, collections\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Text-Fabric\n",
    "\n",
    "Everything starts by setting up Text-Fabric.\n",
    "It needs to know where to look for data.\n",
    "\n",
    "The Syriac texts are in the same repository as this tutorial.\n",
    "I assume you have cloned [linksyr](https://github.com/etcbc/linksyr).\n",
    "in your directory `~/github/etcbc`, so that your directory structure looks like this\n",
    "\n",
    "    your home direcectory\\\n",
    "    |                     - github\\\n",
    "    |                       |      - etcbc\\\n",
    "    |                       |        |         - linksyr\n",
    "    \n",
    "## Tip\n",
    "If you start computing with this tutorial, first copy its parent directory to somewhere else,\n",
    "outside your `linksyr` directory.\n",
    "If you pull changes from the `linksyr` repository later, your work will not be overwritten.\n",
    "Where you put your tutorial directory is up till you.\n",
    "It will work from any directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:45:20.293657Z",
     "start_time": "2018-02-16T15:45:20.277543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.5\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "30 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "REPO = '~/github/etcbc/linksyr'\n",
    "SOURCE = 'syrnt'\n",
    "CORPUS = f'{REPO}/data/tf/{SOURCE}'\n",
    "TF = Fabric(locations=[CORPUS], modules=[''], silent=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Features\n",
    "The data of the corpus is organized in features.\n",
    "They are *columns* of data.\n",
    "Think of the text as a gigantic spreadsheet, where row 1 corresponds to the\n",
    "first word, row 2 to the second word, and so on, for all 100,000+ words.\n",
    "\n",
    "The letters of each word is a column `form` in that spreadsheet.\n",
    "\n",
    "The corpus contains ca. 30 columns, not only for the words, but also for \n",
    "textual objects, such as *books*, *chapters*, and *verses*.\n",
    "\n",
    "Instead of putting that information in one big table, the data is organized in separate columns.\n",
    "We call those columns **features**.\n",
    "\n",
    "We just load the features we need for this tutorial.\n",
    "Later on, where we use them, it will become clear what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:48:45.328269Z",
     "start_time": "2018-02-16T15:48:45.043685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.04s B form                 from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B grammatical_category from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.05s B lexeme               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s Feature overview: 28 for nodes; 1 for edges; 1 configs; 7 computed\n",
      "  0.28s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "    form\n",
    "    grammatical_category\n",
    "    lexeme\n",
    "''')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this all is that we have a bunch of special variables at our disposal\n",
    "that give us access to the text and data of the tablets.\n",
    "\n",
    "At this point it is helpful to throw a quick glance at the text-fabric\n",
    "[API documentation](https://github.com/Dans-labs/text-fabric/wiki/Api)\n",
    "especially the right side bar.\n",
    "\n",
    "The most essential thing for now is that we can use `F` to access the data in the features\n",
    "we've loaded.\n",
    "But there is more, such as `N`, which helps us to walk over the text, as we see in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting\n",
    "\n",
    "In order to get acquainted with the data, we start with the simple task of counting.\n",
    "\n",
    "## Count all nodes\n",
    "We use the \n",
    "[`N()` generator](https://github.com/Dans-labs/text-fabric/wiki/Api#walking-through-nodes)\n",
    "to walk through the nodes.\n",
    "\n",
    "We compared the tablet data to a gigantic spreadsheet, where the rows correspond to the words.\n",
    "In Text-Fabric, we call the rows `slots`, because they are the textual positions that can be filled with words.\n",
    "\n",
    "We also mentioned that there are also other textual objects. \n",
    "They are the tablets, columns, lines, etc.\n",
    "They also correspond to rows in the big spreadsheet.\n",
    "\n",
    "In Text-Fabric we call all these rows *nodes*, and the `N()` generator\n",
    "carries us through those nodes in the textual order.\n",
    "\n",
    "Just one extra thing: the `info` statements generate timed messages.\n",
    "If you use them instead of `print` you'll get a sense of the amount of time that \n",
    "the various processing steps typically need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:48:56.580487Z",
     "start_time": "2018-02-16T15:48:56.531659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Counting nodes ...\n",
      "  0.04s 117884 nodes\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('Counting nodes ...')\n",
    "\n",
    "i = 0\n",
    "for n in N(): i += 1\n",
    "\n",
    "info('{} nodes'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you see it: more than 100,000 nodes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are all those nodes?\n",
    "Every node has a type, like sign, or line, face.\n",
    "We know that we have many of them,\n",
    "but what exactly are they?\n",
    "\n",
    "Text-Fabric has two special features, `otype` and `oslots`, that must occur in every Text-Fabric data set.\n",
    "`otype` tells you for each node its type, and you can ask for the number of `slot`s in the text.\n",
    "\n",
    "Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:49:10.856082Z",
     "start_time": "2018-02-16T15:49:10.834183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.slotType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:49:12.594338Z",
     "start_time": "2018-02-16T15:49:12.586392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109640"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.maxSlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:49:14.357158Z",
     "start_time": "2018-02-16T15:49:14.349597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117884"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.maxNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:49:16.984997Z",
     "start_time": "2018-02-16T15:49:16.979507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('book', 'chapter', 'verse', 'word')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:49:19.273887Z",
     "start_time": "2018-02-16T15:49:19.268839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('book', 4060.740740740741, 109641, 109667),\n",
       " ('chapter', 421.6923076923077, 109668, 109927),\n",
       " ('verse', 13.779062460726404, 109928, 117884),\n",
       " ('word', 1, 1, 109640))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.levels.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting: above you see all the textual objects, with the average size of their objects,\n",
    "the node where they start, and the node where they end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count individual object types\n",
    "This is an intuitive way to count the number of nodes in each type.\n",
    "Note in passing, how we use the `indent` in conjunction with `info` to produce neat timed \n",
    "and indented progress messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:49:27.604573Z",
     "start_time": "2018-02-16T15:49:27.545053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s counting objects ...\n",
      "   |     0.00s      27 books\n",
      "   |     0.00s     260 chapters\n",
      "   |     0.00s    7957 verses\n",
      "   |     0.03s  109640 words\n",
      "  0.04s Done\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('counting objects ...')\n",
    "\n",
    "for otype in F.otype.all:\n",
    "    i = 0\n",
    "\n",
    "    indent(level=1, reset=True)\n",
    "\n",
    "    for n in F.otype.s(otype): i+=1\n",
    "\n",
    "    info('{:>7} {}s'.format(i, otype))\n",
    "\n",
    "indent(level=0)\n",
    "info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature statistics\n",
    "\n",
    "`F`\n",
    "gives access to all features.\n",
    "Every feature has a method\n",
    "`freqList()`\n",
    "to generate a frequency list of its values, higher frequencies first.\n",
    "Here are the lexemes (the top 20):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:49:49.988960Z",
     "start_time": "2018-02-16T15:49:49.905087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('L', 4234),\n",
       " ('HOA', 4006),\n",
       " ('HO', 3397),\n",
       " ('MN', 3335),\n",
       " ('LA', 3140),\n",
       " ('AMR', 2553),\n",
       " ('D;N', 1828),\n",
       " ('EL', 1813),\n",
       " ('ANA', 1728),\n",
       " ('HNA', 1584),\n",
       " ('ANT', 1401),\n",
       " ('CL', 1399),\n",
       " ('ALHA', 1389),\n",
       " ('CD', 1214),\n",
       " (';WOE', 1115),\n",
       " ('A;T', 1100),\n",
       " ('G;R', 1085),\n",
       " ('ATA', 1047),\n",
       " ('A;NA', 858),\n",
       " ('B', 824))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.lexeme.freqList()[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer API\n",
    "We travel upwards and downwards, forwards and backwards through the nodes.\n",
    "The Layer-API (`L`) provides functions: `u()` for going up, and `d()` for going down,\n",
    "`n()` for going to next nodes and `p()` for going to previous nodes.\n",
    "\n",
    "These directions are indirect notions: nodes are just numbers, but by means of the\n",
    "`oslots` feature they are linked to slots. One node *contains* an other node, if the one is linked to a set of slots that contains the set of slots that the other is linked to.\n",
    "And one if next or previous to an other, if its slots follow of precede the slots of the other one.\n",
    "\n",
    "`L.u(node)` **Up** is going to nodes that embed `node`.\n",
    "\n",
    "`L.d(node)` **Down** is the opposite direction, to those that are contained in `node`.\n",
    "\n",
    "`L.n(node)` **Next** are the next *adjacent* nodes, i.e. nodes whose first slot comes immediately after the last slot of `node`.\n",
    "\n",
    "`L.p(node)` **Previous** are the previous *adjacent* nodes, i.e. nodes whose last slot comes immediately before the first slot of `node`.\n",
    "\n",
    "All these functions yield nodes of all possible otypes.\n",
    "By passing an optional parameter, you can restrict the results to nodes of that type.\n",
    "\n",
    "The result are ordered according to the order of things in the text.\n",
    "\n",
    "The functions return always a tuple, even if there is just one node in the result.\n",
    "\n",
    "## Going up\n",
    "We go from the first sign to the tablet it contains.\n",
    "Note the `[0]` at the end. You expect one book, yet `L` returns a tuple. \n",
    "To get the only element of that tuple, you need to do that `[0]`.\n",
    "\n",
    "If you are like me, you keep forgetting it, and that will lead to weird error messages later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:50:11.177878Z",
     "start_time": "2018-02-16T15:50:11.170212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109641\n"
     ]
    }
   ],
   "source": [
    "firstBook = L.u(1, otype='book')[0]\n",
    "print(firstBook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:50:17.780772Z",
     "start_time": "2018-02-16T15:50:17.770317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Matt', 1, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.sectionFromNode(firstBook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see all the containing objects of word 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:50:55.470349Z",
     "start_time": "2018-02-16T15:50:55.457776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 100 is contained in book 109641\n",
      "word 100 is contained in chapter 109668\n",
      "word 100 is contained in verse 109938\n"
     ]
    }
   ],
   "source": [
    "w = 100\n",
    "for otype in F.otype.all:\n",
    "    if otype == F.otype.slotType: continue\n",
    "    up = L.u(w, otype=otype)\n",
    "    upNode = None if len(up) == 0 else up[0]\n",
    "    if upNode is None:\n",
    "        print('word {} is not contained in a {}'.format(w, otype))\n",
    "    else:\n",
    "        print('word {} is contained in {} {}'.format(w, otype, upNode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going next\n",
    "Let's go to the next nodes of the first book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:51:42.341314Z",
     "start_time": "2018-02-16T15:51:42.329480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  13980: word          first slot=13980 , last slot=13980 \n",
      " 110999: verse         first slot=13980 , last slot=13985 \n",
      " 109696: chapter       first slot=13980 , last slot=14490 \n",
      " 109642: book          first slot=13980 , last slot=22772 \n"
     ]
    }
   ],
   "source": [
    "afterFirstBook = L.n(firstBook)\n",
    "for n in afterFirstBook:\n",
    "    print('{:>7}: {:<13} first slot={:<6}, last slot={:<6}'.format(\n",
    "        n, F.otype.v(n),\n",
    "        E.oslots.s(n)[0],\n",
    "        E.oslots.s(n)[-1],\n",
    "    ))\n",
    "secondBook = L.n(firstBook, otype='book')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going previous\n",
    "\n",
    "And let's see what is right before the second book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:52:01.305225Z",
     "start_time": "2018-02-16T15:52:01.296527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 109641: book          first slot=1     , last slot=13979 \n",
      " 109695: chapter       first slot=13714 , last slot=13979 \n",
      " 110998: verse         first slot=13964 , last slot=13979 \n",
      "  13979: word          first slot=13979 , last slot=13979 \n"
     ]
    }
   ],
   "source": [
    "for n in L.p(secondBook):\n",
    "    print('{:>7}: {:<13} first slot={:<6}, last slot={:<6}'.format(\n",
    "        n, F.otype.v(n),\n",
    "        E.oslots.s(n)[0],\n",
    "        E.oslots.s(n)[-1],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go to the chapters of the second book, and just count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:52:29.035567Z",
     "start_time": "2018-02-16T15:52:29.029755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "columns = L.d(secondBook, otype='chapter')\n",
    "print(len(columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first line\n",
    "We pick the first verse and the first word, and explore what is above and below them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:53:24.821042Z",
     "start_time": "2018-02-16T15:53:24.761672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 1\n",
      "   |   UP\n",
      "   |      |   109928          verse\n",
      "   |      |   109668          chapter\n",
      "   |      |   109641          book\n",
      "   |   DOWN\n",
      "   |      |   \n",
      "Node 109928\n",
      "   |   UP\n",
      "   |      |   109668          chapter\n",
      "   |      |   109641          book\n",
      "   |   DOWN\n",
      "   |      |   1               word\n",
      "   |      |   2               word\n",
      "   |      |   3               word\n",
      "   |      |   4               word\n",
      "   |      |   5               word\n",
      "   |      |   6               word\n",
      "   |      |   7               word\n",
      "   |      |   8               word\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "firstVerse = L.d(firstBook, otype='verse')[0]\n",
    "\n",
    "for n in [1, firstVerse]:\n",
    "    indent(level=0)\n",
    "    info('Node {}'.format(n), tm=False)\n",
    "    indent(level=1)\n",
    "    info('UP', tm=False)\n",
    "    indent(level=2)\n",
    "    info('\\n'.join(['{:<15} {}'.format(u, F.otype.v(u)) for u in L.u(n)]), tm=False)\n",
    "    indent(level=1)\n",
    "    info('DOWN', tm=False)\n",
    "    indent(level=2)\n",
    "    info('\\n'.join(['{:<15} {}'.format(u, F.otype.v(u)) for u in L.d(n)]), tm=False)\n",
    "indent(level=0)\n",
    "info('Done', tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text\n",
    "\n",
    "We print the original transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:53:30.062963Z",
     "start_time": "2018-02-16T15:53:30.053201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': {'language': 'default', 'languageEnglish': 'default'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge features: left and right\n",
    "\n",
    "We have not talked about edges much. If the nodes correspond to the rows in the big spreadsheet,\n",
    "the edges point from one row to another.\n",
    "\n",
    "One edge we have encountered: the special feature `oslots`.\n",
    "Each non-slot node is linked by `oslots` to all of its slot nodes.\n",
    "\n",
    "An edge is really a feature as well.\n",
    "Whereas a node feature is a column of information,\n",
    "one cell per node, \n",
    "an edge feature is also a column of information, one cell per pair of nodes.\n",
    "\n",
    "In the tablets quads may be subdivided into subquads and signs, related by operators.\n",
    "If there is an operator `op` between `qLeft` and `qRight`, there is an \n",
    "edge between `qLeft` and `qRight` with value `op`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "By now you have an impression how to compute around in the Hebrew Bible.\n",
    "While this is still the beginning, I hope you already sense the power of unlimited programmatic access\n",
    "to all the bits and bytes in the data set.\n",
    "\n",
    "Here are a few directions for unleashing that power.\n",
    "\n",
    "## Search\n",
    "Text-Fabric contains a flexible search engine, that does not only work for the BHSA data,\n",
    "but also for data that you add to it.\n",
    "There is a tutorial dedicated to [search](search.ipynb).\n",
    "And if you already know MQL queries, you can build from that in\n",
    "[searchFromMQL](searchFromMQL.ipynb).\n",
    "\n",
    "## Explore additional data\n",
    "The ETCBC has a few other repositories with data that work in conjunction with the BHSA data.\n",
    "One of them you have already seen: \n",
    "[phono](https://github.com/ETCBC/phono),\n",
    "for phonetic transcriptions.\n",
    "\n",
    "There is also\n",
    "[parallels](https://github.com/ETCBC/parallels)\n",
    "for detecting parallel passages,\n",
    "and\n",
    "[valence](https://github.com/ETCBC/valence)\n",
    "for studying patterns around verbs that determine their meanings.\n",
    "\n",
    "## Add your own data\n",
    "If you study the additional data, you can observe how that data is created and also\n",
    "how it is turned into a text-fabric data module.\n",
    "The last step is incredibly easy. You can write out every Python dictionary where the keys are numbers\n",
    "and the values string or numbers as a Text-Fabric feature.\n",
    "When you are creating data, you have already constructed those dictionaries, so writing\n",
    "them out is just one method call.\n",
    "See for example how the\n",
    "[flowchart](https://github.com/ETCBC/valence/blob/master/programs/flowchart.ipynb#Add-sense-feature-to-valence-module)\n",
    "notebook in valence writes out verb sense data.\n",
    "![flow](images/valence.png)\n",
    "\n",
    "You can then easily share your new features on GitHub, so that your colleagues everywhere \n",
    "can try it out for themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Emdros MQL\n",
    "\n",
    "[EMDROS](http://emdros.org), written by Ulrik Petersen,\n",
    "is a text database system with the powerful *topographic* query language MQL.\n",
    "The ideas are based on a model devised by Christ-Jan Doedens in\n",
    "[Text Databases: One Database Model and Several Retrieval Languages](https://books.google.nl/books?id=9ggOBRz1dO4C).\n",
    "\n",
    "Text-Fabric's model of slots, nodes and edges is a fairly straightforward translation of the models of Christ-Jan Doedens and Ulrik Petersen.\n",
    "\n",
    "[SHEBANQ](https://shebanq.ancient-data.org) uses EMDROS to offer users to execute and save MQL queries against the Hebrew Text Database of the ETCBC.\n",
    "\n",
    "So it is kind of logical and convenient to be able to work with a Text-Fabric resource through MQL.\n",
    "\n",
    "If you have obtained an MQL dataset somehow, you can turn it into a text-fabric data set by `importMQL()`,\n",
    "which we will not show here.\n",
    "\n",
    "And if you want to export a Text-Fabric data set to MQL, that is also possible.\n",
    "\n",
    "After the `Fabric(modules=...)` call, you can call `exportMQL()` in order to save all features of the\n",
    "indicated modules into a big MQL dump, which can be imported by an EMDROS database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clean caches\n",
    "\n",
    "Text-Fabric pre-computes data for you, so that it can be loaded faster.\n",
    "If the original data is updated, Text-Fabric detects it, and will recompute that data.\n",
    "\n",
    "But there are cases, when the algorithms of Text-Fabric have changed, without any changes in the data, that you might\n",
    "want to clear the cache of precomputed results.\n",
    "\n",
    "There are two ways to do that:\n",
    "\n",
    "* Locate the `.tf` directory of your dataset, and remove all `.tfx` files in it.\n",
    "  This might be a bit awkward to do, because the `.tf` directory is hidden on Unix-like systems.\n",
    "* Call `TF.clearCache()`, which does exactly the same.\n",
    "\n",
    "It is not handy to execute the following cell all the time, that's why I have commented it out.\n",
    "So if you really want to clear the cache, remove the comment sign below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
