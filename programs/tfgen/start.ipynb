{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"images/etcbc.png\"/>\n",
    "<img align=\"right\" src=\"images/peshitta_small.png\"/>\n",
    "<img align=\"right\" src=\"images/tf-small.png\"/>\n",
    "\n",
    "\n",
    "# Tutorial\n",
    "\n",
    "This notebook gets you started with using\n",
    "[Text-Fabric](https://github.com/Dans-labs/text-fabric) for coding in Syriac texts.\n",
    "\n",
    "Chances are that a bit of reading about the underlying\n",
    "[data model](https://github.com/Dans-labs/text-fabric/wiki/Data-model)\n",
    "helps you to follow the exercises below, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most programs start with loading a few modules.\n",
    "In the next cell, the first line loads standard modules that come with Python itself,\n",
    "and the second cell loads Text-Fabric.\n",
    "\n",
    "Before you can run this, you need to install it.\n",
    "The basic instruction for that is, on a terminal:\n",
    "\n",
    "```\n",
    "pip install text-fabric\n",
    "```\n",
    "\n",
    "if you have installed Python with the help of Anaconda, or\n",
    "\n",
    "```\n",
    "sudo -H pip3 install text-fabric\n",
    "```\n",
    "if you have installed Python from [python.org](https://www.python.org).\n",
    "\n",
    "Make sure that you do all this with Python **3**, not 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T08:32:45.194112Z",
     "start_time": "2018-02-17T08:32:45.177545Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os, collections\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Text-Fabric\n",
    "\n",
    "Everything starts by setting up Text-Fabric.\n",
    "It needs to know where to look for data.\n",
    "\n",
    "The Syriac texts are in the same repository as this tutorial.\n",
    "I assume you have cloned [linksyr](https://github.com/etcbc/linksyr).\n",
    "in your directory `~/github/etcbc`, so that your directory structure looks like this\n",
    "\n",
    "    your home direcectory\\\n",
    "    |                     - github\\\n",
    "    |                       |      - etcbc\\\n",
    "    |                       |        |         - linksyr\n",
    "    \n",
    "## Tip\n",
    "If you start computing with this tutorial, first copy its parent directory to somewhere else,\n",
    "outside your `linksyr` directory.\n",
    "If you pull changes from the `linksyr` repository later, your work will not be overwritten.\n",
    "Where you put your tutorial directory is up till you.\n",
    "It will work from any directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:04.851948Z",
     "start_time": "2018-02-17T09:24:04.836708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.5\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "37 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "REPO = '~/github/etcbc/linksyr'\n",
    "SOURCE = 'syrnt'\n",
    "CORPUS = f'{REPO}/data/tf/{SOURCE}'\n",
    "TF = Fabric(locations=[CORPUS], modules=[''], silent=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Features\n",
    "The data of the corpus is organized in features.\n",
    "They are *columns* of data.\n",
    "Think of the text as a gigantic spreadsheet, where row 1 corresponds to the\n",
    "first word, row 2 to the second word, and so on, for all 100,000+ words.\n",
    "\n",
    "The letters of each word is a column `form` in that spreadsheet.\n",
    "\n",
    "The corpus contains ca. 30 columns, not only for the words, but also for \n",
    "textual objects, such as *books*, *chapters*, and *verses*.\n",
    "\n",
    "Instead of putting that information in one big table, the data is organized in separate columns.\n",
    "We call those columns **features**.\n",
    "\n",
    "We just load the features we need for this tutorial.\n",
    "Later on, where we use them, it will become clear what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:06.857007Z",
     "start_time": "2018-02-17T09:24:06.523636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.05s B lexeme               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B lexeme_ascii         from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.03s B grammatical_category from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s Feature overview: 35 for nodes; 1 for edges; 1 configs; 7 computed\n",
      "  0.33s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "    grammatical_category\n",
    "    lexeme lexeme_ascii\n",
    "''')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this all is that we have a bunch of special variables at our disposal\n",
    "that give us access to the text and data of the Hebrew Bible.\n",
    "\n",
    "At this point it is helpful to throw a quick glance at the text-fabric\n",
    "[API documentation](https://github.com/Dans-labs/text-fabric/wiki/Api)\n",
    "especially the right side bar.\n",
    "\n",
    "The most essential thing for now is that we can use `F` to access the data in the features\n",
    "we've loaded.\n",
    "But there is more, such as `N`, which helps us to walk over the text, as we see in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting\n",
    "\n",
    "In order to get acquainted with the data, we start with the simple task of counting.\n",
    "\n",
    "## Count all nodes\n",
    "We use the \n",
    "[`N()` generator](https://github.com/Dans-labs/text-fabric/wiki/Api#walking-through-nodes)\n",
    "to walk through the nodes.\n",
    "\n",
    "We compared corpus to a gigantic spreadsheet, where the rows correspond to the words.\n",
    "In Text-Fabric, we call the rows `slots`, because they are the textual positions that can be filled with words.\n",
    "\n",
    "We also mentioned that there are also more textual objects. \n",
    "They are the verses, chapters and books.\n",
    "They also correspond to rows in the big spreadsheet.\n",
    "\n",
    "In Text-Fabric we call all these rows *nodes*, and the `N()` generator\n",
    "carries us through those nodes in the textual order.\n",
    "\n",
    "Just one extra thing: the `info` statements generate timed messages.\n",
    "If you use them instead of `print` you'll get a sense of the amount of time that \n",
    "the various processing steps typically need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:09.631478Z",
     "start_time": "2018-02-17T09:24:09.595124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Counting nodes ...\n",
      "  0.03s 117884 nodes\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('Counting nodes ...')\n",
    "\n",
    "i = 0\n",
    "for n in N(): i += 1\n",
    "\n",
    "info('{} nodes'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are those nodes?\n",
    "Every node has a type, like word, or phrase, sentence.\n",
    "We know that we have approximately 100,000 words and a few other nodes.\n",
    "But what exactly are they?\n",
    "\n",
    "Text-Fabric has two special features, `otype` and `oslots`, that must occur in every Text-Fabric data set.\n",
    "`otype` tells you for each node its type, and you can ask for the number of `slot`s in the text.\n",
    "\n",
    "Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:11.292340Z",
     "start_time": "2018-02-17T09:24:11.287129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.slotType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:11.694764Z",
     "start_time": "2018-02-17T09:24:11.689401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109640"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.maxSlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:12.046253Z",
     "start_time": "2018-02-17T09:24:12.040964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117884"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.maxNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:12.415824Z",
     "start_time": "2018-02-17T09:24:12.410692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('book', 'chapter', 'verse', 'word')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:12.817268Z",
     "start_time": "2018-02-17T09:24:12.811764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('book', 4060.740740740741, 109641, 109667),\n",
       " ('chapter', 421.6923076923077, 109668, 109927),\n",
       " ('verse', 13.779062460726404, 109928, 117884),\n",
       " ('word', 1, 1, 109640))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.levels.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting: above you see all the textual objects, with the average size of their objects,\n",
    "the node where they start, and the node where they end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count individual object types\n",
    "This is an intuitive way to count the number of nodes in each type.\n",
    "Note in passing, how we use the `indent` in conjunction with `info` to produce neat timed \n",
    "and indented progress messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:15.862088Z",
     "start_time": "2018-02-17T09:24:15.823911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s counting objects ...\n",
      "   |     0.00s      27 books\n",
      "   |     0.00s     260 chapters\n",
      "   |     0.00s    7957 verses\n",
      "   |     0.02s  109640 words\n",
      "  0.03s Done\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('counting objects ...')\n",
    "\n",
    "for otype in F.otype.all:\n",
    "    i = 0\n",
    "    indent(level=1, reset=True)\n",
    "\n",
    "    for n in F.otype.s(otype): i+=1\n",
    "\n",
    "    info('{:>7} {}s'.format(i, otype))\n",
    "\n",
    "indent(level=0)\n",
    "info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature statistics\n",
    "\n",
    "`F`\n",
    "gives access to all features.\n",
    "Every feature has a method\n",
    "`freqList()`\n",
    "to generate a frequency list of its values, higher frequencies first.\n",
    "Here are the parts of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:17.562602Z",
     "start_time": "2018-02-17T09:24:17.496526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('noun', 33717),\n",
       " ('verb', 30441),\n",
       " ('particle', 28486),\n",
       " ('pronoun', 10247),\n",
       " ('adjective', 4503),\n",
       " ('numeral', 1620),\n",
       " ('adverb', 434),\n",
       " ('idiom', 192))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.grammatical_category.freqList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexeme matters\n",
    "\n",
    "## Top 10 frequent verbs\n",
    "\n",
    "If we count the frequency of words, we usually mean the frequency of their\n",
    "corresponding lexemes.\n",
    "\n",
    "There are several methods for working with lexemes.\n",
    "\n",
    "### Method 1: counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:19.141123Z",
     "start_time": "2018-02-17T09:24:19.031492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Collecting data\n",
      "  0.10s Done\n",
      "HOA: 4006\n",
      "AMR: 2553\n",
      "ATA: 965\n",
      "KZA: 734\n",
      "EBD: 706\n",
      ";DE: 704\n",
      "MW;KA: 585\n",
      "XM: 550\n",
      ";HB: 534\n",
      "WME: 494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verbs = collections.Counter()\n",
    "indent(reset=True)\n",
    "info('Collecting data')\n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    if F.grammatical_category.v(w) != 'verb': continue\n",
    "    verbs[F.lexeme_ascii.v(w)] +=1\n",
    "\n",
    "info('Done')\n",
    "print(''.join(\n",
    "    '{}: {}\\n'.format(verb, cnt) for (verb, cnt) in sorted(\n",
    "        verbs.items() , key=lambda x: (-x[1], x[0]))[0:10],\n",
    "    )\n",
    ")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexeme distribution\n",
    "\n",
    "Let's do a bit more fancy lexeme stuff.\n",
    "\n",
    "### Hapaxes\n",
    "\n",
    "A hapax can be found by inspecting lexemes and see to how many word nodes they are linked.\n",
    "If that is number is one, we have a hapax.\n",
    "\n",
    "We print 10 hapaxes with their gloss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:20.520132Z",
     "start_time": "2018-02-17T09:24:20.407629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.10s 835 hapaxes found\n",
      "\t//A\n",
      "\t//LA\n",
      "\t/;DN;A\n",
      "\t/AA\n",
      "\t/ATA\n",
      "\t/D;A\n",
      "\t/IK\n",
      "\t/IKTA\n",
      "\t/KOA\n",
      "\t/LL\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "\n",
    "hapax = []\n",
    "lexIndex = collections.defaultdict(list)\n",
    "\n",
    "for n in F.otype.s('word'):\n",
    "    lexIndex[F.lexeme_ascii.v(n)].append(n)\n",
    "    \n",
    "hapax = dict((lex, occs) for (lex, occs) in lexIndex.items() if len(occs) == 1)\n",
    "    \n",
    "info('{} hapaxes found'.format(len(hapax)))\n",
    "\n",
    "for h in sorted(hapax)[0:10]:\n",
    "    print(f'\\t{h}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want more info on the hapaxes, we get that by means of its *node*.\n",
    "The lexIndex dictionary stores the occurrences of a lexeme as a list of nodes.\n",
    "\n",
    "Let's get the part of speech and the syriac form of those 10 hapaxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:21.781467Z",
     "start_time": "2018-02-17T09:24:21.775430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tnoun         ܨܨܐ\n",
      "\tnoun         ܨܨܠܐ\n",
      "\tadjective    ܨܝܕܢܝܐ\n",
      "\tadjective    ܨܐܐ\n",
      "\tnoun         ܨܐܬܐ\n",
      "\tverb         ܨܕܝܐ\n",
      "\tverb         ܨܦܚ\n",
      "\tnoun         ܨܦܚܬܐ\n",
      "\tnoun         ܨܚܘܐ\n",
      "\tverb         ܨܠܠ\n"
     ]
    }
   ],
   "source": [
    "for h in sorted(hapax)[0:10]:\n",
    "    node = hapax[h][0]\n",
    "    print(f'\\t{F.grammatical_category.v(node):<12} {F.lexeme.v(node)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small occurrence base\n",
    "\n",
    "The occurrence base of a lexeme are the verses, chapters and books in which occurs.\n",
    "Let's look for lexemes that occur in a single chapter.\n",
    "\n",
    "Oh yes, we have already found the hapaxes, we will skip them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:23.790469Z",
     "start_time": "2018-02-17T09:24:23.264344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Finding single chapter lexemes\n",
      "  0.51s 947 single chapter lexemes found\n",
      "Matthew 1:1          ;L;DOTA (1x)\n",
      "Matthew 1:1          ZRK    (1x)\n",
      "Matthew 1:1          TMR    (1x)\n",
      "Matthew 1:1          REOT   (1x)\n",
      "Matthew 1:1          RKBEM  (1x)\n",
      "Matthew 1:1          ;HOWIY (1x)\n",
      "Matthew 1:1          EOZ;A  (1x)\n",
      "Matthew 1:1          ;OTM   (1x)\n",
      "Matthew 1:1          AKZ    (1x)\n",
      "Matthew 1:1          KZX;A  (1x)\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('Finding single chapter lexemes')\n",
    "\n",
    "lexChapterIndex = {}\n",
    "\n",
    "for (lex, occs) in lexIndex.items():\n",
    "    lexChapterIndex[lex] = set(L.u(n, otype='chapter')[0] for n in occs)\n",
    "    \n",
    "singleCh = [(lex, occs) for (lex, occs) in lexChapterIndex.items() if len(lexChapterIndex[lex]) == 1]\n",
    "\n",
    "info('{} single chapter lexemes found'.format(len(singleCh)))\n",
    "\n",
    "for (lex, occs) in singleCh[0:10]:\n",
    "    print('{:<20} {:<6} ({}x)'.format(\n",
    "        '{} {}:{}'.format(*T.sectionFromNode(sorted(occs)[0])),\n",
    "        lex,\n",
    "        len(occs),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confined to books\n",
    "\n",
    "As a final exercise with lexemes, lets make a list of all books, and show their total number of lexemes and\n",
    "the number of lexemes that occur exclusively in that book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:25.912142Z",
     "start_time": "2018-02-17T09:24:25.763992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Making book-lexeme index\n",
      "  0.14s Found 3038 lexemes\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('Making book-lexeme index')\n",
    "\n",
    "allBook = collections.defaultdict(set)\n",
    "allLex = set()\n",
    "\n",
    "for b in F.otype.s('book'):\n",
    "    for w in L.d(b, 'word'):\n",
    "        l = F.lexeme.v(w)\n",
    "        allBook[b].add(l)\n",
    "        allLex.add(l)\n",
    "\n",
    "info('Found {} lexemes'.format(len(allLex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:27.365404Z",
     "start_time": "2018-02-17T09:24:26.748269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Finding single book lexemes\n",
      "  0.60s found 1079 single book lexemes\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('Finding single book lexemes')\n",
    "\n",
    "lexBookIndex = {}\n",
    "\n",
    "for (lex, occs) in lexIndex.items():\n",
    "    lexBookIndex[lex] = set(L.u(n, otype='book')[0] for n in occs)\n",
    "\n",
    "singleBookLex = collections.defaultdict(set)\n",
    "for (lex, books) in lexBookIndex.items():\n",
    "    if len(books) == 1:\n",
    "        singleBookLex[list(books)[0]].add(lex)\n",
    "\n",
    "singleBook = {book: len(lexs) for (book, lexs) in singleBookLex.items()}\n",
    "\n",
    "info('found {} single book lexemes'.format(sum(singleBook.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:29.167912Z",
     "start_time": "2018-02-17T09:24:29.135850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book                 #all #own %own\n",
      "-----------------------------------\n",
      "Acts                 1315  252 19.2%\n",
      "Revelation            809  104 12.9%\n",
      "Luke                 1368  153 11.2%\n",
      "2_Peter               348   34  9.8%\n",
      "Romans                782   60  7.7%\n",
      "2_Timothy             379   26  6.9%\n",
      "Hebrews               747   51  6.8%\n",
      "James                 434   28  6.5%\n",
      "Matthew              1244   80  6.4%\n",
      "John                  810   47  5.8%\n",
      "Jude                  209   12  5.7%\n",
      "2_Corinthians         628   36  5.7%\n",
      "1_Corinthians         740   41  5.5%\n",
      "1_Timothy             444   20  4.5%\n",
      "Philippians           371   15  4.0%\n",
      "Colossians            375   15  4.0%\n",
      "Ephesians             460   18  3.9%\n",
      "1_Peter               452   17  3.8%\n",
      "Mark                  966   32  3.3%\n",
      "2_John                 91    3  3.3%\n",
      "Galatians             418   13  3.1%\n",
      "Titus                 249    6  2.4%\n",
      "3_John                 97    2  2.1%\n",
      "1_Thessalonians       322    6  1.9%\n",
      "Philemon              127    2  1.6%\n",
      "1_John                218    3  1.4%\n",
      "2_Thessalonians       220    3  1.4%\n"
     ]
    }
   ],
   "source": [
    "print('{:<20}{:>5}{:>5}{:>5}\\n{}'.format(\n",
    "    'book', '#all', '#own', '%own',\n",
    "    '-'*35,\n",
    "))\n",
    "booklist = []\n",
    "\n",
    "for b in F.otype.s('book'):\n",
    "    book = T.bookName(b)\n",
    "    a = len(allBook[b])\n",
    "    o = singleBook.get(b, 0)\n",
    "    p = 100 * o / a\n",
    "    booklist.append((book, a, o, p))\n",
    "\n",
    "for x in sorted(booklist, key=lambda e: (-e[3], -e[1], e[0])):\n",
    "    print('{:<20} {:>4} {:>4} {:>4.1f}%'.format(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer API\n",
    "We travel upwards and downwards, forwards and backwards through the nodes.\n",
    "The Layer-API (`L`) provides functions: `u()` for going up, and `d()` for going down,\n",
    "`n()` for going to next nodes and `p()` for going to previous nodes.\n",
    "\n",
    "These directions are indirect notions: nodes are just numbers, but by means of the\n",
    "`oslots` feature they are linked to slots. One node *contains* an other node, if the one is linked to a set of slots that contains the set of slots that the other is linked to.\n",
    "And one if next or previous to an other, if its slots follow of precede the slots of the other one.\n",
    "\n",
    "`L.u(node)` **Up** is going to nodes that embed `node`.\n",
    "\n",
    "`L.d(node)` **Down** is the opposite direction, to those that are contained in `node`.\n",
    "\n",
    "`L.n(node)` **Next** are the next *adjacent* nodes, i.e. nodes whose first slot comes immediately after the last slot of `node`.\n",
    "\n",
    "`L.p(node)` **Previous** are the previous *adjacent* nodes, i.e. nodes whose last slot comes immediately before the first slot of `node`.\n",
    "\n",
    "All these functions yield nodes of all possible otypes.\n",
    "By passing an optional parameter, you can restrict the results to nodes of that type.\n",
    "\n",
    "The result are ordered according to the order of things in the text.\n",
    "\n",
    "The functions return always a tuple, even if there is just one node in the result.\n",
    "\n",
    "## Going up\n",
    "We go from the first word to the book it contains.\n",
    "Note the `[0]` at the end. You expect one book, yet `L` returns a tuple. \n",
    "To get the only element of that tuple, you need to do that `[0]`.\n",
    "\n",
    "If you are like me, you keep forgetting it, and that will lead to weird error messages later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:30.646845Z",
     "start_time": "2018-02-17T09:24:30.641549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109641\n"
     ]
    }
   ],
   "source": [
    "firstBook = L.u(1, otype='book')[0]\n",
    "print(firstBook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see all the containing objects of word 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:32.099515Z",
     "start_time": "2018-02-17T09:24:32.089823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 3 is contained in book 109641\n",
      "word 3 is contained in chapter 109668\n",
      "word 3 is contained in verse 109928\n"
     ]
    }
   ],
   "source": [
    "w = 3\n",
    "for otype in F.otype.all:\n",
    "    if otype == F.otype.slotType: continue\n",
    "    up = L.u(w, otype=otype)\n",
    "    upNode = 'x' if len(up) == 0 else up[0]\n",
    "    print('word {} is contained in {} {}'.format(w, otype, upNode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going next\n",
    "Let's go to the next nodes of the first book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:34.197699Z",
     "start_time": "2018-02-17T09:24:34.190679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  13980: word          first slot=13980 , last slot=13980 \n",
      " 110999: verse         first slot=13980 , last slot=13985 \n",
      " 109696: chapter       first slot=13980 , last slot=14490 \n",
      " 109642: book          first slot=13980 , last slot=22772 \n"
     ]
    }
   ],
   "source": [
    "afterFirstBook = L.n(firstBook)\n",
    "for n in afterFirstBook:\n",
    "    print('{:>7}: {:<13} first slot={:<6}, last slot={:<6}'.format(\n",
    "        n, F.otype.v(n),\n",
    "        E.oslots.s(n)[0],\n",
    "        E.oslots.s(n)[-1],\n",
    "    ))\n",
    "secondBook = L.n(firstBook, otype='book')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going previous\n",
    "\n",
    "And let's see what is right before the second book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:35.512606Z",
     "start_time": "2018-02-17T09:24:35.506886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 109641: book          first slot=1     , last slot=13979 \n",
      " 109695: chapter       first slot=13714 , last slot=13979 \n",
      " 110998: verse         first slot=13964 , last slot=13979 \n",
      "  13979: word          first slot=13979 , last slot=13979 \n"
     ]
    }
   ],
   "source": [
    "for n in L.p(secondBook):\n",
    "    print('{:>7}: {:<13} first slot={:<6}, last slot={:<6}'.format(\n",
    "        n, F.otype.v(n),\n",
    "        E.oslots.s(n)[0],\n",
    "        E.oslots.s(n)[-1],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go to the chapters of the second book, and just count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:37.913617Z",
     "start_time": "2018-02-17T09:24:37.908432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "chapters = L.d(secondBook, otype='chapter')\n",
    "print(len(chapters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first verse\n",
    "We pick the first verse and the first word, and explore what is above and below them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:39.588390Z",
     "start_time": "2018-02-17T09:24:39.551784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 1\n",
      "   |   UP\n",
      "   |      |   109928          verse\n",
      "   |      |   109668          chapter\n",
      "   |      |   109641          book\n",
      "   |   DOWN\n",
      "   |      |   \n",
      "Node 109928\n",
      "   |   UP\n",
      "   |      |   109668          chapter\n",
      "   |      |   109641          book\n",
      "   |   DOWN\n",
      "   |      |   1               word\n",
      "   |      |   2               word\n",
      "   |      |   3               word\n",
      "   |      |   4               word\n",
      "   |      |   5               word\n",
      "   |      |   6               word\n",
      "   |      |   7               word\n",
      "   |      |   8               word\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for n in [1, L.u(1, otype='verse')[0]]:\n",
    "    indent(level=0)\n",
    "    info('Node {}'.format(n), tm=False)\n",
    "    indent(level=1)\n",
    "    info('UP', tm=False)\n",
    "    indent(level=2)\n",
    "    info('\\n'.join(['{:<15} {}'.format(u, F.otype.v(u)) for u in L.u(n)]), tm=False)\n",
    "    indent(level=1)\n",
    "    info('DOWN', tm=False)\n",
    "    indent(level=2)\n",
    "    info('\\n'.join(['{:<15} {}'.format(u, F.otype.v(u)) for u in L.d(n)]), tm=False)\n",
    "indent(level=0)\n",
    "info('Done', tm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text API\n",
    "\n",
    "So far, we have mainly seen nodes and their numbers, and the names of node types.\n",
    "You would almost forget that we are dealing with text.\n",
    "So let's try to see some text.\n",
    "\n",
    "In the same way as `F` gives access to feature data,\n",
    "`T` gives access to the text.\n",
    "That is also feature data, but you can tell Text-Fabric which features are specifically\n",
    "carrying the text, and in return Text-Fabric offers you\n",
    "a Text API: `T`.\n",
    "\n",
    "## Formats\n",
    "Syriac text can be represented in a number of ways:\n",
    "\n",
    "* in transliteration, or in Syriac characters,\n",
    "* showing the actual text or only the lexemes,\n",
    "\n",
    "If you wonder where the information about text formats is stored: \n",
    "not in the program text-fabric, but in the data set.\n",
    "It has a feature `otext`, which specifies the formats and which features\n",
    "must be used to produce them. `otext` is the third special feature in a TF data set,\n",
    "next to `otype` and `oslots`. \n",
    "It is an optional feature. \n",
    "If it is absent, there will be no `T` API.\n",
    "\n",
    "Here is a list of all available formats in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:41.337517Z",
     "start_time": "2018-02-17T09:24:41.331986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lex-orig-full', 'lex-trans-full', 'text-orig-full', 'text-trans-full']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(T.formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the formats\n",
    "Now let's use those formats to print out the first verse of the Hebrew Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:42.877307Z",
     "start_time": "2018-02-17T09:24:42.872602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lex-orig-full:\n",
      "\tܟܬܒܐ ܝܠܝܕܘܬܐ ܝܫܘܥ ܡܫܝܚܐ ܒܪܐ ܕܘܝܕ ܒܪܐ ܐܒܪܗܡ ܐܒܪܗܡ ܝܠܕ ܐܝܣܚܩ \n",
      "lex-trans-full:\n",
      "\tCTBA ;L;DOTA ;WOE MW;KA BRA DO;D BRA ABRHM ABRHM ;LD A;SKX \n",
      "text-orig-full:\n",
      "\tܟܬܒܐ ܕܝܠܝܕܘܬܗ ܕܝܫܘܥ ܡܫܝܚܐ ܒܪܗ ܕܕܘܝܕ ܒܪܗ ܕܐܒܪܗܡ ܐܒܪܗܡ ܐܘܠܕ ܠܐܝܣܚܩ \n",
      "text-trans-full:\n",
      "\tCTBA D;L;DOTH D;WOE MW;KA BRH DDO;D BRH DABRHM ABRHM AOLD LA;SKX \n"
     ]
    }
   ],
   "source": [
    "for fmt in sorted(T.formats):\n",
    "    print('{}:\\n\\t{}'.format(fmt, T.text(range(1,12), fmt=fmt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not specify a format, the **default** format is used (`text-orig-full`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:44.492575Z",
     "start_time": "2018-02-17T09:24:44.488710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ܟܬܒܐ ܕܝܠܝܕܘܬܗ ܕܝܫܘܥ ܡܫܝܚܐ ܒܪܗ ܕܕܘܝܕ ܒܪܗ ܕܐܒܪܗܡ ܐܒܪܗܡ ܐܘܠܕ ܠܐܝܣܚܩ \n"
     ]
    }
   ],
   "source": [
    "print(T.text(range(1,12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole text in all formats in less than a second\n",
    "Part of the pleasure of working with computers is that they can crunch massive amounts of data.\n",
    "The text of the Hebrew Bible is a piece of cake.\n",
    "\n",
    "It takes just ten seconds to have that cake and eat it. \n",
    "In nearly a dozen formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:47.209891Z",
     "start_time": "2018-02-17T09:24:46.411924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s writing plain text of whole Syriac New Testament in all formats\n",
      "  0.78s done 4 formats\n",
      "lex-orig-full\n",
      "ܟܬܒܐ ܝܠܝܕܘܬܐ ܝܫܘܥ ܡܫܝܚܐ ܒܪܐ ܕܘܝܕ ܒܪܐ ܐܒܪܗܡ \n",
      "ܐܒܪܗܡ ܝܠܕ ܐܝܣܚܩ ܐܝܣܚܩ ܝܠܕ ܝܥܩܘܒ ܝܥܩܘܒ ܝܠܕ ܝܗܘܕܐ ܐܚܐ \n",
      "ܝܗܘܕܐ ܝܠܕ ܦܪܨ ܙܪܚ ܡܢ ܬܡܪ ܦܪܨ ܝܠܕ ܚܨܪܘܢ ܚܨܪܘܢ ܝܠܕ ܐܪܡ \n",
      "ܐܪܡ ܝܠܕ ܥܡܝܢܕܒ ܥܡܝܢܕܒ ܝܠܕ ܢܚܫܘܢ ܢܚܫܘܢ ܝܠܕ ܣܠܡܘܢ \n",
      "ܣܠܡܘܢ ܝܠܕ ܒܥܙ ܡܢ ܪܚܒ ܒܥܙ ܝܠܕ ܥܘܒܝܕ ܡܢ ܪܥܘܬ ܥܘܒܝܕ ܝܠܕ ܐܝܫܝ \n",
      "\n",
      "lex-trans-full\n",
      "CTBA ;L;DOTA ;WOE MW;KA BRA DO;D BRA ABRHM \n",
      "ABRHM ;LD A;SKX A;SKX ;LD ;EXOB ;EXOB ;LD ;HODA AKA \n",
      ";HODA ;LD IR/ ZRK MN TMR IR/ ;LD K/RON K/RON ;LD ARM \n",
      "ARM ;LD EM;NDB EM;NDB ;LD NKWON NKWON ;LD SLMON \n",
      "SLMON ;LD BEZ MN RKB BEZ ;LD EOB;D MN REOT EOB;D ;LD A;W; \n",
      "\n",
      "text-orig-full\n",
      "ܟܬܒܐ ܕܝܠܝܕܘܬܗ ܕܝܫܘܥ ܡܫܝܚܐ ܒܪܗ ܕܕܘܝܕ ܒܪܗ ܕܐܒܪܗܡ \n",
      "ܐܒܪܗܡ ܐܘܠܕ ܠܐܝܣܚܩ ܐܝܣܚܩ ܐܘܠܕ ܠܝܥܩܘܒ ܝܥܩܘܒ ܐܘܠܕ ܠܝܗܘܕܐ ܘܠܐܚܘܗܝ \n",
      "ܝܗܘܕܐ ܐܘܠܕ ܠܦܪܨ ܘܠܙܪܚ ܡܢ ܬܡܪ ܦܪܨ ܐܘܠܕ ܠܚܨܪܘܢ ܚܨܪܘܢ ܐܘܠܕ ܠܐܪܡ \n",
      "ܐܪܡ ܐܘܠܕ ܠܥܡܝܢܕܒ ܥܡܝܢܕܒ ܐܘܠܕ ܠܢܚܫܘܢ ܢܚܫܘܢ ܐܘܠܕ ܠܣܠܡܘܢ \n",
      "ܣܠܡܘܢ ܐܘܠܕ ܠܒܥܙ ܡܢ ܪܚܒ ܒܥܙ ܐܘܠܕ ܠܥܘܒܝܕ ܡܢ ܪܥܘܬ ܥܘܒܝܕ ܐܘܠܕ ܠܐܝܫܝ \n",
      "\n",
      "text-trans-full\n",
      "CTBA D;L;DOTH D;WOE MW;KA BRH DDO;D BRH DABRHM \n",
      "ABRHM AOLD LA;SKX A;SKX AOLD L;EXOB ;EXOB AOLD L;HODA OLAKOH; \n",
      ";HODA AOLD LIR/ OLZRK MN TMR IR/ AOLD LK/RON K/RON AOLD LARM \n",
      "ARM AOLD LEM;NDB EM;NDB AOLD LNKWON NKWON AOLD LSLMON \n",
      "SLMON AOLD LBEZ MN RKB BEZ AOLD LEOB;D MN REOT EOB;D AOLD LA;W; \n",
      "\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('writing plain text of whole Syriac New Testament in all formats')\n",
    "\n",
    "text = collections.defaultdict(list)\n",
    "\n",
    "for v in F.otype.s('verse'):\n",
    "    words = L.d(v, 'word')\n",
    "    for fmt in sorted(T.formats):\n",
    "        text[fmt].append(T.text(words, fmt=fmt))\n",
    "\n",
    "info('done {} formats'.format(len(text)))\n",
    "\n",
    "for fmt in sorted(text):\n",
    "    print('{}\\n{}\\n'.format(fmt, '\\n'.join(text[fmt][0:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full plain text\n",
    "We write a few formats to file, in your `Downloads` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:49.078899Z",
     "start_time": "2018-02-17T09:24:49.064507Z"
    }
   },
   "outputs": [],
   "source": [
    "orig = 'text-orig-full'\n",
    "trans = 'text-trans-full'\n",
    "for fmt in (orig, trans):\n",
    "    with open(os.path.expanduser(f'~/Downloads/{fmt}.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(text[fmt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:50.128589Z",
     "start_time": "2018-02-17T09:24:50.000320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ܟܬܒܐ ܕܝܠܝܕܘܬܗ ܕܝܫܘܥ ܡܫܝܚܐ ܒܪܗ ܕܕܘܝܕ ܒܪܗ ܕܐܒܪܗܡ \r\n",
      "ܐܒܪܗܡ ܐܘܠܕ ܠܐܝܣܚܩ ܐܝܣܚܩ ܐܘܠܕ ܠܝܥܩܘܒ ܝܥܩܘܒ ܐܘܠܕ ܠܝܗܘܕܐ ܘܠܐܚܘܗܝ \r\n",
      "ܝܗܘܕܐ ܐܘܠܕ ܠܦܪܨ ܘܠܙܪܚ ܡܢ ܬܡܪ ܦܪܨ ܐܘܠܕ ܠܚܨܪܘܢ ܚܨܪܘܢ ܐܘܠܕ ܠܐܪܡ \r\n",
      "ܐܪܡ ܐܘܠܕ ܠܥܡܝܢܕܒ ܥܡܝܢܕܒ ܐܘܠܕ ܠܢܚܫܘܢ ܢܚܫܘܢ ܐܘܠܕ ܠܣܠܡܘܢ \r\n",
      "ܣܠܡܘܢ ܐܘܠܕ ܠܒܥܙ ܡܢ ܪܚܒ ܒܥܙ ܐܘܠܕ ܠܥܘܒܝܕ ܡܢ ܪܥܘܬ ܥܘܒܝܕ ܐܘܠܕ ܠܐܝܫܝ \r\n",
      "ܐܝܫܝ ܐܘܠܕ ܠܕܘܝܕ ܡܠܟܐ ܕܘܝܕ ܐܘܠܕ ܠܫܠܝܡܘܢ ܡܢ ܐܢܬܬܗ ܕܐܘܪܝܐ \r\n",
      "ܫܠܝܡܘܢ ܐܘܠܕ ܠܪܚܒܥܡ ܪܚܒܥܡ ܐܘܠܕ ܠܐܒܝܐ ܐܒܝܐ ܐܘܠܕ ܠܐܣܐ \r\n",
      "ܐܣܐ ܐܘܠܕ ܠܝܗܘܫܦܛ ܝܗܘܫܦܛ ܐܘܠܕ ܠܝܘܪܡ ܝܘܪܡ ܐܘܠܕ ܠܥܘܙܝܐ \r\n",
      "ܥܘܙܝܐ ܐܘܠܕ ܠܝܘܬܡ ܝܘܬܡ ܐܘܠܕ ܠܐܚܙ ܐܚܙ ܐܘܠܕ ܠܚܙܩܝܐ \r\n",
      "ܚܙܩܝܐ ܐܘܠܕ ܠܡܢܫܐ ܡܢܫܐ ܐܘܠܕ ܠܐܡܘܢ ܐܡܘܢ ܐܘܠܕ ܠܝܘܫܝܐ \r\n",
      "ܝܘܫܝܐ ܐܘܠܕ ܠܝܘܟܢܝܐ ܘܠܐܚܘܗܝ ܒܓܠܘܬܐ ܕܒܒܠ \r\n",
      "ܡܢ ܒܬܪ ܓܠܘܬܐ ܕܝܢ ܕܒܒܠ ܝܘܟܢܝܐ ܐܘܠܕ ܠܫܠܬܐܝܠ ܫܠܬܐܝܠ ܐܘܠܕ ܠܙܘܪܒܒܠ \r\n",
      "ܙܘܪܒܒܠ ܐܘܠܕ ܠܐܒܝܘܕ ܐܒܝܘܕ ܐܘܠܕ ܠܐܠܝܩܝܡ ܐܠܝܩܝܡ ܐܘܠܕ ܠܥܙܘܪ \r\n",
      "ܥܙܘܪ ܐܘܠܕ ܠܙܕܘܩ ܙܕܘܩ ܐܘܠܕ ܠܐܟܝܢ ܐܟܝܢ ܐܘܠܕ ܠܐܠܝܘܕ \r\n",
      "ܐܠܝܘܕ ܐܘܠܕ ܠܐܠܝܥܙܪ ܐܠܝܥܙܪ ܐܘܠܕ ܠܡܬܢ ܡܬܢ ܐܘܠܕ ܠܝܥܩܘܒ \r\n",
      "ܝܥܩܘܒ ܐܘܠܕ ܠܝܘܣܦ ܓܒܪܗ ܕܡܪܝܡ ܕܡܢܗ ܐܬܝܠܕ ܝܫܘܥ ܕܡܬܩܪܐ ܡܫܝܚܐ \r\n",
      "ܟܠܗܝܢ ܗܟܝܠ ܫܪܒܬܐ ܡܢ ܐܒܪܗܡ ܥܕܡܐ ܠܕܘܝܕ ܫܪܒܬܐ ܐܪܒܥܣܪܐ ܘܡܢ ܕܘܝܕ ܥܕܡܐ ܠܓܠܘܬܐ ܕܒܒܠ ܫܪܒܬܐ ܐܪܒܥܣܪܐ ܘܡܢ ܓܠܘܬܐ ܕܒܒܠ ܥܕܡܐ ܠܡܫܝܚܐ ܫܪܒܬܐ ܐܪܒܥܣܪܐ \r\n",
      "ܝܠܕܗ ܕܝܢ ܕܝܫܘܥ ܡܫܝܚܐ ܗܟܢܐ ܗܘܐ ܟܕ ܡܟܝܪܐ ܗܘܬ ܡܪܝܡ ܐܡܗ ܠܝܘܣܦ ܥܕܠܐ ܢܫܬܘܬܦܘܢ ܐܫܬܟܚܬ ܒܛܢܐ ܡܢ ܪܘܚܐ ܕܩܘܕܫܐ \r\n",
      "ܝܘܣܦ ܕܝܢ ܒܥܠܗ ܟܐܢܐ ܗܘܐ ܘܠܐ ܨܒܐ ܕܢܦܪܣܝܗ ܘܐܬܪܥܝ ܗܘܐ ܕܡܛܫܝܐܝܬ ܢܫܪܝܗ \r\n",
      "ܟܕ ܗܠܝܢ ܕܝܢ ܐܬܪܥܝ ܐܬܚܙܝ ܠܗ ܡܠܐܟܐ ܕܡܪܝܐ ܒܚܠܡܐ ܘܐܡܪ ܠܗ ܝܘܣܦ ܒܪܗ ܕܕܘܝܕ ܠܐ ܬܕܚܠ ܠܡܣܒ ܠܡܪܝܡ ܐܢܬܬܟ ܗܘ ܓܝܪ ܕܐܬܝܠܕ ܒܗ ܡܢ ܪܘܚܐ ܗܘ ܕܩܘܕܫܐ \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 ~/Downloads/{orig}.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book names\n",
    "\n",
    "For Bible book names, we can use several languages.\n",
    "Well, in this case we have just English.\n",
    "\n",
    "### Languages\n",
    "Here are the languages that we can use for book names.\n",
    "These languages come from the features `book@ll`, where `ll` is a two letter\n",
    "ISO language code. Have a look in your data directory, you can't miss them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:24:52.439840Z",
     "start_time": "2018-02-17T09:24:52.433174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': {'language': 'default', 'languageEnglish': 'default'},\n",
       " 'en': {'language': 'English', 'languageEnglish': 'English'}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "\n",
    "A section is a book, a chapter or a verse.\n",
    "Knowledge of sections is not baked into Text-Fabric. \n",
    "The config feature `otext.tf` may specify three section levels, and tell\n",
    "what the corresponding node types and features are.\n",
    "\n",
    "From that knowledge it can construct mappings from nodes to sections, e.g. from verse\n",
    "nodes to tuples of the form:\n",
    "\n",
    "    (bookName, chapterNumber, verseNumber)\n",
    "   \n",
    "Here are examples of getting the section that corresponds to a node and vice versa.\n",
    "\n",
    "**NB:** `sectionFromNode` always delivers a verse specification, either from the\n",
    "first slot belonging to that node, or, if `lastSlot`, from the last slot\n",
    "belonging to that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:28:57.596363Z",
     "start_time": "2018-02-17T09:28:57.575887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section of first word          ('Matthew', 1, 1)\n",
      "node of Matthew 1:1            109928\n",
      "node of book Matthew           109641\n",
      "node of chapter Matthew 1      109668\n",
      "section of book node           ('Matthew', 1, 1)\n",
      "idem, now last word            ('Matthew', 28, 20)\n",
      "section of chapter node        ('Matthew', 1, 1)\n",
      "idem, now last word            ('Matthew', 1, 25)\n"
     ]
    }
   ],
   "source": [
    "for x in (\n",
    "    ('section of first word',     T.sectionFromNode(1)                            ),\n",
    "    ('node of Matthew 1:1',       T.nodeFromSection(('Matthew', 1, 1))            ),\n",
    "    ('node of book Matthew',      T.nodeFromSection(('Matthew',))                 ),\n",
    "    ('node of chapter Matthew 1', T.nodeFromSection(('Matthew', 1))               ),\n",
    "    ('section of book node',      T.sectionFromNode(109641)                      ),\n",
    "    ('idem, now last word',       T.sectionFromNode(109641, lastSlot=True)       ),\n",
    "    ('section of chapter node',   T.sectionFromNode(109668)                      ),\n",
    "    ('idem, now last word',       T.sectionFromNode(109668, lastSlot=True)       ),\n",
    "): print('{:<30} {}'.format(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "By now you have an impression how to compute around in the text.\n",
    "While this is still the beginning, I hope you already sense the power of unlimited programmatic access\n",
    "to all the bits and bytes in the data set.\n",
    "\n",
    "Here are a few directions for unleashing that power.\n",
    "\n",
    "## Search\n",
    "Text-Fabric contains a flexible search engine, that does not only work for this data,\n",
    "but also for data that you add to it.\n",
    "There is a tutorial dedicated to [search](search.ipynb).\n",
    "And if you already know MQL queries, you can build from that in\n",
    "[searchFromMQL](searchFromMQL.ipynb).\n",
    "\n",
    "\n",
    "## Add your own data\n",
    "If you study the additional data, you can observe how that data is created and also\n",
    "how it is turned into a text-fabric data module.\n",
    "The last step is incredibly easy. You can write out every Python dictionary where the keys are numbers\n",
    "and the values string or numbers as a Text-Fabric feature.\n",
    "When you are creating data, you have already constructed those dictionaries, so writing\n",
    "them out is just one method call.\n",
    "See for example how the\n",
    "[flowchart](https://github.com/ETCBC/valence/blob/master/programs/flowchart.ipynb#Add-sense-feature-to-valence-module)\n",
    "notebook in valence writes out verb sense data.\n",
    "![flow](images/valence.png)\n",
    "\n",
    "You can then easily share your new features on GitHub, so that your colleagues everywhere \n",
    "can try it out for themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Emdros MQL\n",
    "\n",
    "[EMDROS](http://emdros.org), written by Ulrik Petersen,\n",
    "is a text database system with the powerful *topographic* query language MQL.\n",
    "The ideas are based on a model devised by Christ-Jan Doedens in\n",
    "[Text Databases: One Database Model and Several Retrieval Languages](https://books.google.nl/books?id=9ggOBRz1dO4C).\n",
    "\n",
    "Text-Fabric's model of slots, nodes and edges is a fairly straightforward translation of the models of Christ-Jan Doedens and Ulrik Petersen.\n",
    "\n",
    "[SHEBANQ](https://shebanq.ancient-data.org) uses EMDROS to offer users to execute and save MQL queries against the Hebrew Text Database of the ETCBC.\n",
    "\n",
    "So it is kind of logical and convenient to be able to work with a Text-Fabric resource through MQL.\n",
    "\n",
    "If you have obtained an MQL dataset somehow, you can turn it into a text-fabric data set by `importMQL()`,\n",
    "which we will not show here.\n",
    "\n",
    "And if you want to export a Text-Fabric data set to MQL, that is also possible.\n",
    "\n",
    "After the `Fabric(modules=...)` call, you can call `exportMQL()` in order to save all features of the\n",
    "indicated modules into a big MQL dump, which can be imported by an EMDROS database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T09:30:44.433713Z",
     "start_time": "2018-02-17T09:30:36.563177Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Checking features of dataset mysygnt\n",
      "   |     0.00s M aspect               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   |     0.00s feature \"book@en\" => \"book_en\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     0.00s M demonstrative_category from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M feminine_he_dot      from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M gender               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M noun_type            from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M number               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M numeral_type         from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M participle_type      from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M person               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M prefix               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M prefix_ascii         from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M pronoun_type         from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M root                 from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M root_ascii           from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M seyame               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M state                from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M stem                 from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M stem_ascii           from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M suffix               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M suffix_ascii         from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M suffix_contraction   from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M suffix_gender        from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M suffix_number        from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M suffix_person        from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.00s M verbal_conjugation   from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "  0.07s 34 features to export to MQL ...\n",
      "  0.07s Loading 34 features\n",
      "   |     0.08s B aspect               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.05s B demonstrative_category from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.02s B feminine_he_dot      from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.03s B gender               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B noun_type            from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B number               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B numeral_type         from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B participle_type      from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.03s B person               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.03s B prefix               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.02s B prefix_ascii         from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B pronoun_type         from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.05s B root                 from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B root_ascii           from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.02s B seyame               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B state                from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.06s B stem                 from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B stem_ascii           from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.03s B suffix               from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.02s B suffix_ascii         from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B suffix_contraction   from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.03s B suffix_gender        from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B suffix_number        from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.03s B suffix_person        from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "   |     0.04s B verbal_conjugation   from /Users/dirk/github/etcbc/linksyr/data/tf/syrnt\n",
      "  1.02s Writing enumerations\n",
      "\taspect         :    6 values, 1 not a name, e.g. «n/a»\n",
      "\tbook           :   27 values, 11 not a name, e.g. «1Cor»\n",
      "\tbook_en        :   27 values, 11 not a name, e.g. «1_Corinthians»\n",
      "\tdemonstrative_category:    3 values, 1 not a name, e.g. «n/a»\n",
      "\tgender         :    4 values, 1 not a name, e.g. «n/a»\n",
      "\tnoun_type      :    3 values, 1 not a name, e.g. «n/a»\n",
      "\tnumber         :    3 values, 1 not a name, e.g. «n/a»\n",
      "\tnumeral_type   :    2 values, 1 not a name, e.g. «n/a»\n",
      "\tparticiple_type:    3 values, 1 not a name, e.g. «n/a»\n",
      "\tperson         :    4 values, 4 not a name, e.g. «1»\n",
      "\tprefix         :   19 values, 19 not a name, e.g. «»\n",
      "\tprefix_ascii   :   19 values, 1 not a name, e.g. «»\n",
      "\tpronoun_type   :    4 values, 1 not a name, e.g. «n/a»\n",
      "\tseyame         :    2 values, 2 not a name, e.g. «0»\n",
      "\tstate          :    4 values, 1 not a name, e.g. «n/a»\n",
      "\tsuffix         :   51 values, 51 not a name, e.g. «»\n",
      "\tsuffix_ascii   :   51 values, 31 not a name, e.g. «»\n",
      "\tsuffix_contraction:    3 values, 1 not a name, e.g. «n/a»\n",
      "\tsuffix_gender  :    3 values, 1 not a name, e.g. «c/n/a»\n",
      "\tsuffix_number  :    2 values, 1 not a name, e.g. «s/n/a»\n",
      "\tsuffix_person  :    4 values, 4 not a name, e.g. «1»\n",
      "\tverbal_conjugation:   23 values, 1 not a name, e.g. «n/a»\n",
      "   |     0.30s Writing an all-in-one enum with    8 values\n",
      "  1.33s Mapping 34 features onto 4 object types\n",
      "  1.87s Writing 34 features as data in 4 object types\n",
      "   |     0.00s word data ...\n",
      "   |      |     2.98s batch of size               30.8MB with   50000 of   50000 words\n",
      "   |      |     5.39s batch of size               30.9MB with   50000 of  100000 words\n",
      "   |      |     5.87s batch of size                6.0MB with    9640 of  109640 words\n",
      "   |     5.87s word data: 109640 objects\n",
      "   |     0.00s verse data ...\n",
      "   |      |     0.07s batch of size              595.6KB with    7957 of    7957 verses\n",
      "   |     0.07s verse data: 7957 objects\n",
      "   |     0.00s chapter data ...\n",
      "   |      |     0.02s batch of size               19.9KB with     260 of     260 chapters\n",
      "   |     0.02s chapter data: 260 objects\n",
      "   |     0.00s book data ...\n",
      "   |      |     0.02s batch of size                2.7KB with      27 of      27 books\n",
      "   |     0.02s book data: 27 objects\n",
      "  7.86s Done\n"
     ]
    }
   ],
   "source": [
    "TF.exportMQL('mysygnt','~/Downloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a file `~/Downloads/mysygnt.mql` of 72 MB.\n",
    "You can import it into an Emdros database by saying:\n",
    "\n",
    "    cd ~/Downloads\n",
    "    rm mysygnt\n",
    "    mql -b 3 < mysygnt.mql\n",
    "    \n",
    "The result is an SQLite3 database `mysygnt` in the same directory (17 MB).\n",
    "You can run a query against it by creating a text file test.mql with this contents:\n",
    "\n",
    "    select all objects where\n",
    "    [verse\n",
    "        [word FOCUS lexeme_ascii = 'WME']\n",
    "    ]\n",
    "    \n",
    "And then say\n",
    "\n",
    "    mql -b 3 -d mysygnt test.mql\n",
    "    \n",
    "You will see raw query results: all word occurrences that belong to lexemes with `make` in their gloss.\n",
    "     \n",
    "It is not very pretty, and probably you should use a more visual Emdros tool to run those queries.\n",
    "You see a lot of node numbers, but the good thing is, you can look those node numbers up in Text-Fabric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clean caches\n",
    "\n",
    "Text-Fabric pre-computes data for you, so that it can be loaded faster.\n",
    "If the original data is updated, Text-Fabric detects it, and will recompute that data.\n",
    "\n",
    "But there are cases, when the algorithms of Text-Fabric have changed, without any changes in the data, that you might\n",
    "want to clear the cache of precomputed results.\n",
    "\n",
    "There are two ways to do that:\n",
    "\n",
    "* Locate the `.tf` directory of your dataset, and remove all `.tfx` files in it.\n",
    "  This might be a bit awkward to do, because the `.tf` directory is hidden on Unix-like systems.\n",
    "* Call `TF.clearCache()`, which does exactly the same.\n",
    "\n",
    "It is not handy to execute the following cell all the time, that's why I have commented it out.\n",
    "So if you really want to clear the cache, remove the comment sign below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
